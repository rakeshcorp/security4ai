Databricks AI Security Framework

Data operations
- Raw data
- Data preparation
- Datasets
- Catalog and governance
20 specific risks:
1.1 Insufficient access controls: Access control, secure & private network access, control data access/objects and securely share it, monitor audit logs.
1.2 Missing data classification:  Classify data with tags as it’s ingested into the platform aligning with the organization’s governance requirements.
1.3 Poor data quality: Data quality checks, monitor data and AI system using single pane of glass and setup alerts.
1.4 Ineffective storage and encryption: Control access to data/objects, encryption of data at rest/in-transit.
1.5 Lack of data versioning: Version data and track change logs on large-scale datasets that are fed to your models.
1.6 Insufficient data lineage: Capture and view data lineage.
1.7 Lack of data trustworthiness: Version data and track change logs on data that are fed to your models, securely share data/AI assets.
1.8 Legality of data: Right to be forgotten, pretrain LLM to only use allowed data for inference. Track models/data lineage in model retraining.
1.9 Stale data: Use data quality checks, use near real-time data.
1.10 Lack of data access logs: Audit actions on datasets, monitor audit logs.
1.11 Compromised third-party datasets: Control access to data/objects, data quality checks, data lineage.
2.1 Preprocessing integrity: Access control, secure & private network access, control data access/objects, data quality checks, data lineage, secure model features, source control, monitor audit logs.
2.2 Feature manipulation: Access control, secure & private network access, Secure model features to prevent and track unauthorized updates to features and for lineage or traceability.
2.3 Raw data criteria: Access control, secure & private network access, Use access control lists to control access to data.
2.4 Adversarial partitions: Ensure traceability by tracking and reproducing training data partitions, linking models and runs to specific datasets, and holding human owners accountable for ML model training.
3.1 Data poisoning: Access control, secure & private network access, control data access/objects, data quality checks, data lineage, secure model features, source control, monitor audit logs.
3.2 Ineffective storage and encryption: Control access to data/objects, encryption of data at rest/in-transit.
3.3 Label flipping: Control access to data/objects, encryption of data at rest/in-transit.
4.1 Lack of traceability and transparency of model assets: Control access to data/objects, data quality checks, data lineage, Govern model assets for traceability.
4.2 Lack of end-to-end ML lifecycle: Manage end-to-end ML lifecycle for measuring, versioning, tracking model artifacts, monitoring metrics and results.



Model operations
- ML algorithm
- Evaluation
- Model build
- Model management
15 specific risks:
5.1 Lack of tracking and reproducibility of experiments: Track ML training runs for measuring, versioning, tracking model artifacts/algorithms, training environment, hyperparameters, metrics and results.
5.2 Model drift:  Secure model features to track changes to features, monitor data and AI system for these changes for further actions.
5.3 Hyperparameters stealing:  Track ML training runs in the model development process, including parameter settings, securely. User access control lists.
5.4 Malicious libraries: Third-party library control to limit the potential for malicious third-party libraries and code.
6.1 Evaluation data poisoning: Access control for data/objects, secure & private network access, data quality checks, data lineage, Automate LLM evaluation to capture performance insights.
6.2 Insufficient evaluation data: Build models on clean, representative data, leveraging RAG for LLMs, and continuously evaluating outputs through performance monitoring and prompt-based comparisons.
6.3 Lack of interpretability and explainability: performance tracking, monitoring, explainability tools (e.g., LIME, SHAP), model cards, red teaming, and human-in-the-loop oversight.
7.1 Backdoor machine learning/Trojaned model: Pretrain LLMs on in-house data, continuously scanning third-party models for risks in CI/CD, hardening runtimes, controlling libraries, and Sandboxing.
7.2 Model assets leak: Access control, secure & private network access, control data access/objects, securely manage credentials of data sources/assets.
7.3 ML supply chain vulnerabilities: Pretrain LLMs on in-house data, continuously scanning third-party models for risks in CI/CD, hardening runtimes, controlling libraries, and Sandboxing.
7.4 Source code control attack: Source code control, Security scan for third-party libraries, sandbox testing. 
8.1 Model attribution: Create model aliases, tags and annotations for documenting and discovering models. Control data access and secure sharing.
8.2 Model theft: Encrypt models, secure model serving endpoints, securely manage credentials, rate limit APIs, manage securely LLM providers, audit logs and monitoring.
8.3 Model lifecycle without HITL: Control access to data, objects, models & model assets with human in the loop oversight on permissions, versions and approvals to promote models to production.
8.4 Model inversion: Access control, secure network access, encrypt models, secure model serving endpoints, securely manage credentials, rate limit APIs, manage securely LLM providers, audit logs and monitoring.


Model deployment and serving
- Model Serving —inference requests
- Model Serving — inference responses
19 specific risks:
9.1 Prompt injection: Access control, secure model serving endpoints, implement AI guardrails, robust system prompts, rate limiting, strict input and output validation to reject invalid queries.
9.2 Model inversion: Access control, encrypt models, secure model serving endpoints, implement AI guardrails, robust system prompts, rate limiting, strict input and output validation to reject invalid queries.
9.3 Model breakout: Run models in isolated, least-privileged/isolated environments, sanitize outputs, parameterize plug-ins, and enforce least-privilege for external calls.
9.4 Looped input: Set up inference tables to log model requests and responses, enabling monitoring, debugging, and evaluation of inference quality for potential retraining.
9.5 Infer training data membership: Access control, encrypt models, secure model serving endpoints, implement AI guardrails, robust system prompts, rate limiting, encrypt models, model evaluation.
9.6 Discover ML model ontology: Access control, encrypt models, secure model serving endpoints, implement AI guardrails, robust system prompts, rate limiting, encrypt models, model aliases/tags & annotations.
9.7 Denial of service (DOS): Access control, secure network access, encrypt models, secure model serving endpoints, implement AI guardrails, robust system prompts, rate limiting, encrypt models.
9.8 LLM hallucinations: Use techniques such as RAG, Fine tuning or pre-training on highly relevant, contextual data. Automate LLM evaluation (eg. LLM as a judge) and setup guardrails.
9.9 Input resource control: Access control, secure network access, setup AI guardrails, Store and retrieve embeddings securely.
9.10 Accidental exposure of unauthorized data to models: Attribute-based access control, protect data with filters/masking. Secure model features and monitoring audit logs.
9.11 Model Inference API access: Secure network and Access control, secure model serving endpoints, manage credentials securely, monitoring audit logs.
9.12 LLM jailbreak: Access control, secure network access, setup AI guardrails, secure model serving endpoints, limit access from models and AI agents.
9.13 Excessive agency: Data classification, data lineage, attribute-based access control, protect data with filters/masking, limit access from models and AI agents, network segmentation.
10.1 Lack of audit and monitoring inference quality:  Track model performance to evaluate quality, Set up inference tables for monitoring and debugging models, monitoring audit logs.
10.2 Output manipulation: Secure model serving endpoints with encryption in transit & access control, rate limiting, robust system prompts and output validation.
10.3 Discover ML model ontology: Access control, encrypt models, secure model serving endpoints, implement AI guardrails, robust system prompts, rate limiting, encrypt models, model aliases/tags & annotations.
10.4 Discover ML model family: Access control, encrypt models, secure model serving endpoints, implement AI guardrails, robust system prompts, rate limiting, encrypt models, model aliases/tags & annotations.
10.5 Black box attacks: Secure model serving endpoints with encryption in transit & access control, rate limiting, implement AI guardrails.
10.6 Sensitive data output from a model: protect data with filters/masking, Secure model serving endpoints, implement AI guardrails, rate limiting.


Operations and platform
- ML operations
- ML platform
8 specific risks:
11.1 Lack of MLOps — repeatable enforced standards: Employ data-centric MLOps and LLMOps best practices. Trigger actions to keep human-in-the-loop. Evaluate models to capture performance insights.
12.1 Lack of vulnerability management: Platform security — penetration testing, red teaming, bug bounty and vulnerability management and remediation including SLA.
12.2 Lack of penetration testing, red teaming and bug bounty: Platform security — Incident Reponse Team. A bug bounty program implementation.
12.3 Lack of incident response: Incident response team leadership, team strcture, incident response plan and testing.
12.4 Unauthorized privileged access: Privileged management access (PAM) program and implementation, governance.
12.5 Poor SDLC: Secure SDLC practices and culture. Integrate secuirty in design and CI/CD process.
12.6 Lack of compliance: Platform compliance to build on a compliant platform. Compliance requirements, continuous evaluation and reporting.
12.7 Initial access: Secure user and network access controls, control access to data & objects, secure model serving endpoints and monitoring audit logs.


Reeference: https://www.databricks.com/sites/default/files/2025-02/databricks-ebook-dasf-2.pdf
